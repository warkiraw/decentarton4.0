"""
–ú–æ–¥—É–ª—å feature engineering –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RFM-D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ propensity scores.
"""

import pandas as pd
import numpy as np
import logging
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score
from typing import Dict, Any
from config import CONFIG

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_rfmd_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–µ—Ç RFM-D (Recency, Frequency, Monetary, Diversity) –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤.
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
        
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ RFM-D –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ RFM-D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    df_features = df.copy()
    
    # 1. Recency - –¥–∞–≤–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    # –ü–æ—Å–∫–æ–ª—å–∫—É —É –Ω–∞—Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∫–∞–∫ –±–∞–∑—É
    current_date = pd.Timestamp.now()
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º recency
    # –î–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å–ª–æ–≤–Ω—É—é recency –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–ª–∞–Ω—Å–∞
    # –í—ã—Å–æ–∫–∏–π –±–∞–ª–∞–Ω—Å = –Ω–µ–¥–∞–≤–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∏–π recency)
    if 'avg_monthly_balance_KZT' in df_features.columns:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º recency (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –≤—ã—Å–æ–∫–∏–π –±–∞–ª–∞–Ω—Å = –Ω–∏–∑–∫–∏–π recency)
        max_balance = df_features['avg_monthly_balance_KZT'].max()
        if max_balance > 0:
            df_features['recency'] = 1 - (df_features['avg_monthly_balance_KZT'] / max_balance)
        else:
            df_features['recency'] = 0.5
    else:
        df_features['recency'] = 0.5
    
    # 2. Frequency - —á–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞—Ç –≤ —Ñ–æ–∫—É—Å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
    focus_categories = ['–¢–∞–∫—Å–∏', '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', '–†–µ—Å—Ç–æ—Ä–∞–Ω', '–ü–æ–∫—É–ø–∫–∏', '–†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è']
    frequency_score = 0
    
    for category in focus_categories:
        spend_col = f'spend_{category}'
        if spend_col in df_features.columns:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–Ω—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
            frequency_score += (df_features[spend_col] > 0).astype(int)
    
    df_features['frequency'] = frequency_score
    
    # 3. Monetary - —Å—Ä–µ–¥–Ω–µ–º–µ—Å—è—á–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã + –æ—Å—Ç–∞—Ç–æ–∫
    monetary_score = df_features.get('avg_monthly_balance_KZT', 0)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Ç—Ä–∞—Ç—ã –ø–æ –≤—Å–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    spend_columns = [col for col in df_features.columns if col.startswith('spend_')]
    if spend_columns:
        total_spending = df_features[spend_columns].sum(axis=1)
        monetary_score += total_spending
    
    df_features['monetary'] = monetary_score
    
    # 4. Diversity - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Ç–∏–ø–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    diversity_score = 0
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
    spend_categories = [col for col in df_features.columns if col.startswith('spend_')]
    if spend_categories:
        diversity_score += (df_features[spend_categories] > 0).sum(axis=1)
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
    transfer_types = [col for col in df_features.columns if col.startswith('transfer_')]
    if transfer_types:
        diversity_score += (df_features[transfer_types] > 0).sum(axis=1)
    
    df_features['diversity'] = diversity_score
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = MinMaxScaler()
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º recency (—É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º frequency
    if df_features['frequency'].max() > 0:
        df_features['frequency_normalized'] = df_features['frequency'] / df_features['frequency'].max()
    else:
        df_features['frequency_normalized'] = 0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º monetary
    if df_features['monetary'].max() > 0:
        df_features['monetary_normalized'] = scaler.fit_transform(df_features[['monetary']])[:, 0]
    else:
        df_features['monetary_normalized'] = 0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º diversity
    if df_features['diversity'].max() > 0:
        df_features['diversity_normalized'] = df_features['diversity'] / df_features['diversity'].max()
    else:
        df_features['diversity_normalized'] = 0
    
    logger.info(f"RFM-D –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è {len(df_features)} –∫–ª–∏–µ–Ω—Ç–æ–≤")
    
    return df_features


def add_cluster_labels(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    üöÄ –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å elbow method –∏ advanced features –¥–ª—è >0.4 silhouette!
    
    Args:
        df: DataFrame —Å RFM-D –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        config: –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """
    logger.info("üöÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–û–ô –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤")
    
    df_clustered = df.copy()
    
    # 1. üéØ –†–ê–°–®–ò–†–ï–ù–ù–´–ï –§–ò–ß–ò –¥–ª—è –ª—É—á—à–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    advanced_features = []
    
    # –ë–∞–∑–æ–≤—ã–µ RFM-D (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ)
    base_features = ['recency', 'frequency_normalized', 'monetary_normalized', 'diversity_normalized']
    for feature in base_features:
        if feature in df_clustered.columns:
            advanced_features.append(feature)
    
    # –ï—Å–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ
    if not advanced_features:
        fallback_features = ['recency', 'frequency', 'monetary', 'diversity']
        for feature in fallback_features:
            if feature in df_clustered.columns:
                advanced_features.append(feature)
    
    # üí∞ Spending patterns - –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç—Ä–∞—Ç
    spend_columns = [col for col in df_clustered.columns if col.startswith('spend_')]
    if len(spend_columns) > 3:
        # –î–æ–±–∞–≤–ª—è–µ–º –¢–û–ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç—Ä–∞—Ç
        advanced_features.extend(spend_columns[:8])  # –¢–æ–ø-8 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        
        # üìä –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–∞—Ç (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–∞)  
        df_clustered['spend_volatility'] = df_clustered[spend_columns].std(axis=1)
        advanced_features.append('spend_volatility')
        
        # üéØ –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –≤ —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        top3_sums = df_clustered[spend_columns].apply(lambda x: x.nlargest(3).sum(), axis=1)
        total_sums = df_clustered[spend_columns].sum(axis=1)
        df_clustered['top3_concentration'] = np.where(total_sums > 0, top3_sums / total_sums, 0)
        advanced_features.append('top3_concentration')
    
    # üí± FX –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    fx_cols = [col for col in df_clustered.columns if 'fx_' in col.lower()]
    if fx_cols:
        df_clustered['fx_activity_total'] = df_clustered[fx_cols].sum(axis=1) 
        advanced_features.append('fx_activity_total')
    
    # üè¶ –ë–∞–ª–∞–Ω—Å (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
    if 'avg_monthly_balance_KZT' in df_clustered.columns:
        df_clustered['balance_log'] = np.log1p(df_clustered['avg_monthly_balance_KZT'])
        advanced_features.append('balance_log')
    
    # üß† Propensity scores –µ—Å–ª–∏ –µ—Å—Ç—å
    propensity_cols = [col for col in df_clustered.columns if col.startswith('propensity_')]
    advanced_features.extend(propensity_cols)
    
    # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
    existing_features = [col for col in advanced_features if col in df_clustered.columns]
    
    if len(existing_features) < 3:
        logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ–∏—á–µ–π –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        df_clustered['cluster'] = 0
        return df_clustered
    
    logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(existing_features)} –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ–∏—á–µ–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    
    # 2. üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    features_for_clustering = df_clustered[existing_features].fillna(0)
    
    # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ —Ñ–∏—á–∏
    feature_std = features_for_clustering.std()
    variable_features = feature_std[feature_std > 0].index.tolist()
    
    if len(variable_features) < 2:
        logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–∏—á–µ–π")
        df_clustered['cluster'] = 0
        return df_clustered
    
    features_for_clustering = features_for_clustering[variable_features]
    
    # 3. üî¨ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    try:
        scaled_features = scaler.fit_transform(features_for_clustering)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏: {e}")
        df_clustered['cluster'] = 0
        return df_clustered
    
    # 4. üìà ELBOW METHOD –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    max_clusters = min(8, len(df_clustered) // 4)  # –†–∞–∑—É–º–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
    k_range = range(2, max_clusters + 1)
    
    silhouette_scores = []
    inertias = []
    
    for k in k_range:
        try:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(scaled_features)
            inertias.append(kmeans.inertia_)
            
            if len(set(labels)) > 1:
                silhouette_avg = silhouette_score(scaled_features, labels)
                silhouette_scores.append(silhouette_avg)
            else:
                silhouette_scores.append(0)
        except Exception:
            silhouette_scores.append(0)
            inertias.append(float('inf'))
    
    # üéØ –í—ã–±–∏—Ä–∞–µ–º k —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º silhouette (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤—É!)
    if silhouette_scores and max(silhouette_scores) > 0:
        optimal_k = k_range[np.argmax(silhouette_scores)]
        expected_silhouette = max(silhouette_scores)
    else:
        optimal_k = config.get('CLUSTER_PARAMS', {}).get('n_clusters', 4)
        expected_silhouette = 0
    
    logger.info(f"üèÜ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ k: {optimal_k}, –æ–∂–∏–¥–∞–µ–º—ã–π silhouette: {expected_silhouette:.3f}")
    
    # 5. üöÄ –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º k
    try:
        final_kmeans = KMeans(
            n_clusters=optimal_k, 
            random_state=42, 
            n_init=20,  # –ë–æ–ª—å—à–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            max_iter=500
        )
        cluster_labels = final_kmeans.fit_predict(scaled_features)
        df_clustered['cluster'] = cluster_labels
        
        # 6. üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if len(set(cluster_labels)) > 1:
            final_silhouette = silhouette_score(scaled_features, cluster_labels)
            logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π Silhouette Score: {final_silhouette:.3f}")
            
            if final_silhouette >= 0.4:
                logger.info(f"üèÜ –ü–û–ë–ï–î–ê! –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Ü–µ–ª–µ–≤–æ–π Silhouette: {final_silhouette:.3f} >= 0.4!")
            else:
                logger.warning(f"‚ö†Ô∏è Silhouette –Ω–∏–∂–µ —Ü–µ–ª–µ–≤–æ–≥–æ: {final_silhouette:.3f} < 0.4")
        else:
            logger.warning("–í—Å–µ –æ–±—ä–µ–∫—Ç—ã –ø–æ–ø–∞–ª–∏ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        cluster_distribution = df_clustered['cluster'].value_counts().sort_index()
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {dict(cluster_distribution)}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        df_clustered['cluster'] = 0
    
    return df_clustered


def add_propensity_scores(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç propensity scores –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ AB_TEST_FLAG == 'B'.
    
    Args:
        df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        config: –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ propensity scores
    """
    if config.get('AB_TEST_FLAG', 'A') != 'B':
        logger.info("Propensity scores –æ—Ç–∫–ª—é—á–µ–Ω—ã (AB_TEST_FLAG != 'B')")
        return df
    
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ propensity scores –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
    
    df_propensity = df.copy()
    
    # –ü—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è propensity
    target_products = ['–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞', '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–î–µ–ø–æ–∑–∏—Ç –º—É–ª—å—Ç–∏–≤–∞–ª—é—Ç–Ω—ã–π']
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_columns = ['recency', 'frequency_normalized', 'monetary_normalized', 'diversity_normalized']
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_features = []
    if 'status' in df_propensity.columns:
        categorical_features.append('status')
    if 'city' in df_propensity.columns:
        categorical_features.append('city')
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    spend_columns = [col for col in df_propensity.columns if col.startswith('spend_')]
    feature_columns.extend(spend_columns)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
    transfer_columns = [col for col in df_propensity.columns if col.startswith('transfer_')]
    feature_columns.extend(transfer_columns)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
    existing_numeric_features = [col for col in feature_columns if col in df_propensity.columns]
    existing_categorical_features = [col for col in categorical_features if col in df_propensity.columns]
    
    if not existing_numeric_features:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è propensity –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
        for product in target_products:
            safe_product_name = product.lower().replace(' ', '_').replace('—ë', '–µ')
            df_propensity[f'propensity_{safe_product_name}'] = 0.5
        return df_propensity
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    for product in target_products:
        try:
            safe_product_name = product.lower().replace(' ', '_').replace('—ë', '–µ')
            
            # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ç–∞—Ä–≥–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç—Ä–∞—Ç
            target = _create_synthetic_target(df_propensity, product, config)
            
            if target.sum() == 0 or target.sum() == len(target):
                # –ï—Å–ª–∏ –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π propensity
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç –¥–ª—è {product}")
                df_propensity[f'propensity_{safe_product_name}'] = 0.5
                continue
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_numeric = df_propensity[existing_numeric_features].fillna(0)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if existing_categorical_features:
                encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
                X_categorical = encoder.fit_transform(
                    df_propensity[existing_categorical_features].fillna('unknown')
                )
                X = np.hstack([X_numeric.values, X_categorical])
            else:
                X = X_numeric.values
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, target,
                test_size=1 - config['PROPENSITY_PARAMS']['train_split'],
                random_state=42,
                stratify=target if len(set(target)) > 1 else None
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—É–≤–µ–ª–∏—á–µ–Ω max_iter –¥–ª—è convergence)
            lr = LogisticRegression(random_state=42, max_iter=5000, solver='lbfgs')
            lr.fit(X_train, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ propensity scores
            propensity_scores = lr.predict_proba(X)[:, 1]
            df_propensity[f'propensity_{safe_product_name}'] = propensity_scores
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            train_score = lr.score(X_train, y_train)
            test_score = lr.score(X_test, y_test)
            logger.info(f"Propensity –º–æ–¥–µ–ª—å –¥–ª—è {product}: Train={train_score:.3f}, Test={test_score:.3f}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ propensity –º–æ–¥–µ–ª–∏ –¥–ª—è {product}: {e}")
            df_propensity[f'propensity_{safe_product_name}'] = 0.5
    
    return df_propensity


def _create_synthetic_target(df: pd.DataFrame, product: str, config: Dict[str, Any]) -> np.ndarray:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ç–∞—Ä–≥–µ—Ç –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è.
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
        product: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        –ú–∞—Å—Å–∏–≤ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç–∞—Ä–≥–µ—Ç–∞–º–∏ (0 –∏–ª–∏ 1)
    """
    threshold_spend = config['PROPENSITY_PARAMS']['threshold_spend']
    targets = np.zeros(len(df))
    
    if product == '–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞':
        # –§–ò–ö–°: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ –¥–∞–Ω–Ω—ã—Ö + –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ bool
        restaurant_spend = df.get('spend_–ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã', 0)
        online_spend = (df.get('spend_–ï–¥–∏–º –¥–æ–º–∞', 0) + 
                       df.get('spend_–°–º–æ—Ç—Ä–∏–º –¥–æ–º–∞', 0) + 
                       df.get('spend_–ò–≥—Ä–∞–µ–º –¥–æ–º–∞', 0))
        total_spend = restaurant_spend + online_spend
        condition = total_spend > threshold_spend * 0.3
        targets = np.array(condition, dtype=int)  # –§–ò–ö–°: –∏—Å–ø–æ–ª—å–∑—É–µ–º np.array –≤–º–µ—Å—Ç–æ .astype(int)
        
    elif product == '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏':
        # –ö–ª–∏–µ–Ω—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º–∏ –±–∞–ª–∞–Ω—Å–∞–º–∏ –∏ –Ω–∏–∑–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é —Ç—Ä–∞—Ç
        high_balance = df.get('avg_monthly_balance_KZT', 0) > config['RFMD_THRESHOLDS']['high_balance']
        spend_cols = [col for col in df.columns if col.startswith('spend_')]
        if spend_cols:
            low_spending = df[spend_cols].sum(axis=1) < threshold_spend * 0.5
        else:
            low_spending = pd.Series([True] * len(df), index=df.index)
        condition = high_balance & low_spending
        
        # –§–ò–ö–°: –î–æ–±–∞–≤–ª—è–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –µ—Å–ª–∏ target –Ω–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π
        targets = np.array(condition, dtype=int)
        if targets.sum() == 0 or targets.sum() == len(targets):
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (10% —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–ª–∏–ø–æ–≤)
            np.random.seed(42)
            noise_indices = np.random.choice(len(targets), size=max(1, len(targets)//10), replace=False)
            targets[noise_indices] = 1 - targets[noise_indices]
        
    elif product == '–î–µ–ø–æ–∑–∏—Ç –º—É–ª—å—Ç–∏–≤–∞–ª—é—Ç–Ω—ã–π':
        # –ö–ª–∏–µ–Ω—Ç—ã —Å FX –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
        fx_buy = df.get('transfer_fx_buy', 0)
        fx_sell = df.get('transfer_fx_sell', 0)
        fx_activity = fx_buy + fx_sell
        condition = fx_activity > config['RFMD_THRESHOLDS']['fx_volume_threshold']
        targets = np.array(condition, dtype=int)  # –§–ò–ö–°: –∏—Å–ø–æ–ª—å–∑—É–µ–º np.array –≤–º–µ—Å—Ç–æ .astype(int)
    
    return targets


def test_create_rfmd_features():
    """–¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è RFM-D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_data = pd.DataFrame({
            'client_code': [1, 2, 3],
            'avg_monthly_balance_KZT': [100000, 200000, 50000],
            'spend_–¢–∞–∫—Å–∏': [5000, 0, 2000],
            'spend_–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è': [50000, 30000, 0],
            'spend_–†–µ—Å—Ç–æ—Ä–∞–Ω': [15000, 20000, 8000],
            'transfer_fx_buy': [10000, 0, 5000],
            'transfer_fx_sell': [0, 20000, 0]
        })
        
        result = create_rfmd_features(test_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        required_columns = ['recency', 'frequency', 'monetary', 'diversity']
        for col in required_columns:
            assert col in result.columns, f"–°—Ç–æ–ª–±–µ—Ü {col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
        assert result['recency'].between(0, 1).all(), "Recency –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1"
        assert result['frequency'].min() >= 0, "Frequency –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º"
        assert result['monetary'].min() >= 0, "Monetary –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º"
        assert result['diversity'].min() >= 0, "Diversity –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º"
        
        print("–¢–µ—Å—Ç create_rfmd_features –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"–¢–µ—Å—Ç create_rfmd_features –Ω–µ –ø—Ä–æ–π–¥–µ–Ω: {e}")


if __name__ == "__main__":
    test_create_rfmd_features()